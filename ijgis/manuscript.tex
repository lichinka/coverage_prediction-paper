%% LyX 2.0.5.1 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[twocolumn,english]{tGIS2e}
\usepackage[T1]{fontenc}
\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{3}
\usepackage{babel}
\usepackage{prettyref}
\usepackage{booktabs}
\usepackage{amstext}
\usepackage{graphicx}
\usepackage[authoryear]{natbib}
\usepackage[unicode=true,
 bookmarks=true,bookmarksnumbered=true,bookmarksopen=true,bookmarksopenlevel=1,
 breaklinks=false,pdfborder={0 0 0},backref=false,colorlinks=false]
 {hyperref}
\hypersetup{
 pdfpagelayout=OneColumn, pdfnewwindow=true, pdfstartview=XYZ, plainpages=false}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
%% Because html converters don't know tabularnewline
\providecommand{\tabularnewline}{\\}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
%
% Format of Figure cross-references
%
\usepackage{prettyref}
\newrefformat{fig}{Figure~\ref{#1}}

\makeatother

\begin{document}
\doi{10.1080/1365881YYxxxxxxxx}

\title{A GRASS GIS parallel module for radio-propagation predictions}



\begin{abstract}

Geographic information systems are ideal candidates for applying parallel
programming techniques, mainly because of the large data sets they
usually handle. To help dealing with complex calculations over such
data sets, we investigate the challenges and advantages of applying
a work-pool parallel paradigm over a message-passing communication
model. To this end, we present the design and implementation of a
parallel radio-coverage prediction tool for the GRASS environment.
The prediction calculation employs digital elevation models and land-usage
data in order to analyze the radio coverage of a geographic area.
We provide an extended analysis of the experimental results, which
are based on real data from an LTE network currently deployed in Slovenia.
According to the experiments, which are performed on a computer cluster,
the presented methods allow the parallel radio-coverage prediction
tool to reach great scalability, meaning it is able to tackle real-world-sized
data sets while greatly reducing the processing time and maximizing
hardware utilization. Moreover, we are able to solve problem instances,
which sizes are out of reach of the reference implementation.

\end{abstract}

\begin{keywords}

GRASS, GIS, parallel, LTE, simulation.

\end{keywords}

\maketitle


\section{Introduction}

Although the well-known and often cited Gordon Moore's prediction
still holds \citep{Moore_Cramming_more_components_onto_integrated_circuits:1998},
the fact is that for the last few years, CPU speed has hardly been
improving. Instead, the number of cores within a single CPU is growing.
This fact poses a challenge for software development in general and
research in particular: a hardware upgrade will, most of the time,
fail to deliver twice the computing capacity of its predecessor. However,
since this commodity hardware is present in practically all modern
desktop computers, it creates an opportunity for the exploitation
of these parallel computing resources to enhance the performance of
complex algorithms over large data sets. The challenge is thus to
deliver the computing power of multi-core systems into a geographic
information systems (GIS). Moreover, by accessing many of such computing
nodes through a network connection, the palette of possibilities broadens.

A traditional approach when dealing with computationally expensive
problem solving is to simplify the models in order to be able to execute
their calculations within a feasible amount of time. Clearly, this
increases the introduced error level, which is not an option for a
certain group of simulations, e.g. those dealing with disaster contingency
planning and decision support \citep{Huang_Using_adaptively_coupled_models_and_high_performance_computing_for_enabling_the_computability_of_dust_storm_forecasting:2012,Yin_A_framework_for_integrating_GIS_and_parallel_computing_for_spatial_control_problems_a_case_study_of_wildfire_dontrol:2012}.
The conducted simulations during the planning phase of a radio network
also belong to this group. Their results are the basis for the decision
making prior physically installing the base stations and antennas
that will cover a certain geographical area. A higher deviation of
these results increases the probability of making the wrong decisions
at installation time, which may unnecessarily increase costs or even
cause losses to mobile network operators.

A group of studies has successfully deployed high-performance computing
(HPC) systems and techniques to solve different problems dealing with
spatial data \citep{Armstrong_Using_a_computational_grid_for_geographic_information_analysis:2005,Li_Parallel_cellular_automata_for_large_scale_urban_simulation_using_load_balancing_techniques:2010,Guan_A_parallel_computing_approach_to_fast_geostatistical_areal_interpolation:2011,Yin_A_framework_for_integrating_GIS_and_parallel_computing_for_spatial_control_problems_a_case_study_of_wildfire_dontrol:2012,Osterman_CUDA_on_GRASS:2012,Tabik_Simultaneous_computation_of_total_viewshed_on_large_high_resolution_grids:2012,Widener_Developing_a_parallel_computational_implementation_of_AMOEBA:2012}.
Unfortunately, despite some rare exceptions, most of these works are
application-specific and do not introduce general principles for joining
GIS and HPC. There are several reasons for this, but the most widely
known fact is that parallel programming and HPC often call for area
experts in order to integrate these practices in a given environment
\citep{Clematis_High_performance_computing_with_geographical_data:2003}.
Moreover, the wide range of options currently available create even
more barriers for general users willing to reach HPC.





In this paper, we combine proved principles of HPC and introduce some
new approaches in order to improve the performance speed of a GIS
module for radio-propagation predictions. To this end, we implement
a parallel radio-prediction tool for the open source Geographic Resources
Analysis Support System (GRASS) \citep{Neteler_Open_source_GIS_a_GRASS_GIS_approach}.
For its architecture, we have focused on scalability, clean design
and openness of the tool, inspired by the GRASS GIS. These facts make
it an ideal candidate for showing the benefits of the presented patterns,
while tackling radio-coverage predictions of big problem instances,
e.g. real mobile networks containing thousands of transmitters over
high-resolution terrains, optimization problems which require a large
number of coverage evaluations, and big scale whole-country-sized
simulations. 

Within the introduced context, the provided guidelines may be used
as a template for parallelization of other computationally-expensive
tasks within the GRASS environment. Namely, the radio-coverage prediction
problem tackled in this work presents some common points with many
different problems within geographic information sciences. Each transmitter
within the radio network works as a point of interest, around which
certain phenomena is analyzed. As an example, think about wild fire
control, where the fire starts at several points over a given area
\citep{Yin_A_framework_for_integrating_GIS_and_parallel_computing_for_spatial_control_problems_a_case_study_of_wildfire_dontrol:2012}.
Some weather-related analysis also follows this pattern \citep{Huang_Using_adaptively_coupled_models_and_high_performance_computing_for_enabling_the_computability_of_dust_storm_forecasting:2012}.
Similarly, agent-based simulations present several analogies, where
the points of interest represent the deployed agents \citep{Gong_Parallel_agent_based_simulation_of_individual_level_spatial_interactions_within_a_multicore_computing_environment:2012}.


\subsection{Parallel computation on computer clusters}

Considering the high computational power needed for predicting the
radio-coverage of a real mobile network, the use of a computer cluster
is required. A computer cluster is a group of interconnected computers
that work together as a single system. To reach high levels of parallel
performance and scalability, this work discusses the key steps of
parallel decomposition of big data sets, and the distribution of the
computational load among the computing nodes that belong to the cluster.
As a practical instance for introducing the afore-mentioned principles,
we use the radio-coverage prediction problem for real mobile networks.

Computer clusters typically consist of several commodity PCs connected
through a high-speed local network with a distributed file system,
like NFS \citep{Shepler_Network_file_system:2003}. One such system
is the DEGIMA cluster \citep{Hamada_Cluster_of_GPUs:2010} at the
Nagasaki Advanced Computing Center (NACC) of the Nagasaki University
in Japan. This system ranked in the TOP 500 list of supercomputers
until June 2012%
\footnote{http://www.top500.org%
}, and in June 2011 held the third place of the Green 500 list%
\footnote{http://www.green500.org%
} as one of the most energy-efficient supercomputers in the world.

It is worth pointing out that the condition of the cluster working
as a single system is not necessary in the context of this study.
The introduced principles work equally well over any set of networked
computers. The decision of using the DEGIMA cluster is related with
the requirement of simulating large runs in order to assert the benefits
and drawbacks of the presented methodology.


\subsection{Objectives\label{sub:Objectives}}



As a proof-of-concept of the presented patterns, we develop a high-performance
parallel radio prediction tool (PRATO) targeted at large real-world
network environments, such as the ones currently deployed by several
mobile operators around the world. Therefore, our focus is on the
performance and scalability of PRATO, while other more dynamic aspects
of radio networks are not considered. Among these aspects are code
distributions, details of handover, and dynamics related to radio
resource management \citep{Khan_LTE_for_4G_mobile_broadband_air_interface_technologies_and_performance:2009}.

The performance evaluation of PRATO in a distributed computing environment
is a major objective of this work, since its implementation is founded
on the parallel methods presented in this study. Furthermore, by presenting
a detailed description of the design and implementation of PRATO,
we provide guidelines and patterns for achieving higher efficiency
levels for general task parallelization in GRASS GIS. Additionally,
we introduce techniques to overcome several obstacles encountered
during our research, as well as in related work, which significantly
improve the performance and lower the complexity of the presented
implementation, e.g. the inability to use GRASS in a threaded environment,
lowering overhead of I/O operations, saving simulation results asynchronously
and independently from GRASS, and improving load balancing with an
asynchronous message-passing technique.

The paper is organized as follows. Section \ref{sec:Description-of-the-radio-coverage-prediction-tool}
gives a description of the radio-coverage prediction problem, including
the radio-propagation model. Section \ref{sec:Design-and-implementation}
concentrates on the design principles and implementation details of
the radio propagation tool, for the serial and parallel versions.
Section \ref{sec:Simulations} discusses the experimental results
and their analysis. Finally, Section \ref{sec:Related-work} gives
an overview of relevant publications, describing how they relate to
our work, before drawing some conclusions.


\section{Radio-coverage prediction for mobile networks \label{sec:Description-of-the-radio-coverage-prediction-tool}}


\subsection{Background}

The coverage planning of radio networks remains a key problem that
all mobile operators have to deal with. Moreover, it has proven to
be a fundamental issue not only in LTE networks, but also in other
standards for mobile communications \citep{Saleh_On_the_coveraga_extension_in_LTE_networks:2010,Shabbir_Comparison_of_radio_propagation_models:2011,Siomina:Minimum.pilot.power.for.service.coverage,Valcarce_Applying.FDTD.to.the.coverage.prediction.of.WiMAX:2009}.
One of the primary objectives of mobile-network planning is to efficiently
use the allocated frequency band to assure that some geographic area
of interest can be satisfactorily reached with the base stations of
the network. To this end, radio-coverage prediction tools are of great
importance as they allow network engineers to test different network
configurations before physically implementing the changes. Nevertheless,
radio-coverage prediction is a complex task, mainly due to the wide
range of various combinations of hardware and configuration parameters
that have to be analyzed in the context of different environments.
The complexity of the problem means that radio-coverage prediction
is a computationally-intensive and time-consuming task, hence the
importance of using fast and accurate tools. Additionally, since the
number of deployed transmitters keeps growing with the adoption of
modern standards \citep{Saleh_On_the_coveraga_extension_in_LTE_networks:2010},
there is a clear need for a radio propagation tool that is able to
cope with larger work loads in a feasible amount of time.



PRATO is a high-performance radio-prediction tool for GSM (2G), UMTS
(3G) and LTE (4G) radio networks. It is implemented as a module for
the GRASS GIS. It can be used for planning the different phases of
a new radio-network installation, as well as a support tool for maintenance
activities related to network troubleshooting or upgrading.

As a reference implementation, we have used the publicly available
radio coverage prediction tool, developed in \citep{Ozimek_Open.source.radio.coverage.prediction:2010}.
The authors of this work have developed a modular radio coverage tool
that performs separate calculations for radio-signal path loss and
antenna radiation patterns, also taking into account different configuration
parameters, such as antenna tilting, azimuth and height. The output
result, saved as a raster map, is the maximum signal level over the
target area, in which each point represents the received signal from
the best serving transmitter. This work implements some well-known
radio propagation models, e.g. Okumura-Hata \citet{Hata_Empirical_formula_for_propagation_loss_in_land_mobile_radio_services:1980}
and COST 231 \citet{Cichon_Propagation.prediction.models:1995}, the
later is explained in more detail in Section \ref{sub:COST-231-model}.
Regarding the accuracy of the predicted values, the authors \citep{Ozimek_Open.source.radio.coverage.prediction:2010}
report comparable results to those of a state-of-the-art commercial
tool. Furthermore, to ensure that our implementation is completely
compliant with the afore-mentioned reference, we have designed a comparison
test that consists of running both tools with the same set of input
parameters. The test results from PRATO and the reference implementation
were identical in all tested cases.


\subsection{Propagation modeling\label{sub:COST-231-model}}

The COST-231 Walfisch-Ikegami radio-propagation model was introduced
as an extension of the well-known COST Hata model \citep{Sarkar_Survey_of_radio_propagation_models:2003,Shabbir_Comparison_of_radio_propagation_models:2011}.
The suitability of this model comes from the fact that it distinguishes
between line-of-sight (LOS) and non-line-of-sight (NLOS) conditions.

In this work, as well as in the reference implementation \citep{Ozimek_Open.source.radio.coverage.prediction:2010},
the terrain profile is used for LOS determination. The wave-guide
effect in streets of big cities is not taken into account, because
the building data is not available. In order to compensate the missing
data, we include a correction factor, based on the land usage (clutter
data). This technique is also adopted by other propagation models
for radio networks, like the artificial neural networks macro-cell
model developed in \citep{Neskovic_Microcell_electric_field_strength_prediction_model:2010}.
Consequently, we introduce an extra term for signal loss due to clutter
($L_{\textrm{CLUT}}$), thus defining the LOS and NLOS path losses
as

\begin{equation}
PL_{\textrm{LOS}}(d)=42.64+26\log(d)+20\log(F)+L_{\textrm{CLUT}}\label{eq:cost231_LOS-1}
\end{equation}
and

\begin{equation}
PL_{\textrm{NLOS}}(d)=L_{0}+L_{\textrm{RTS}}+L_{\textrm{MSD}}+L_{\textrm{CLUT}},\label{eq:cost231_NLOS-1}
\end{equation}


\noindent where $d$ is the distance (in kilometers) from the transmitter
to the receiver point, $F$ is the frequency (in MHz), $L_{0}$ is
the attenuation in free space (in dB), $L_{\textrm{RTS}}$ represents
the diffraction from roof top to street, and $L_{\textrm{MSD}}$ represents
the diffraction loss due to multiple obstacles.

Equation (\ref{eq:cost231_LOS-1}) describes the path loss when there
is LOS between the transmitter and the receiver. On the other hand,
in NLOS conditions, the path loss is calculated as in Equation (\ref{eq:cost231_NLOS-1}).


\section{Design and implementation \label{sec:Design-and-implementation}}




\subsection{Design of the serial version}

This section describes the different functions contained in the serial
version of PRATO, which is implemented as a GRASS module. Their connections
and data flow are depicted in \prettyref{fig:serial_version_flow_diagram},
where the parallelograms of the flow diagram represent input/output
(I/O) operations. 

Our design follows a similar internal organization as the radio planning
tool presented in \citep{Ozimek_Open.source.radio.coverage.prediction:2010},
but with some essential differences. Specifically, our approach employs
a direct connection to an external database server for intermediate
result saving, instead of the slow built-in GRASS database drivers.
To explicitly avoid tight coupling with a specific database vendor,
the generated output is formatted in plain text, which is then forwarded
to the database server. Any further processing is achieved by issuing
a query over the database tables that contain the partial results
for each of the processed transmitters. 



\begin{figure}[tbh]
\centering

\includegraphics[width=1\columnwidth]{img/serial_implementation_flow_diagram}

\caption{\textit{\emph{Flow diagram of the serial version.}}\textit{\label{fig:serial_version_flow_diagram}}}
\end{figure}



\subsubsection{Isotropic path-loss calculation\label{sub:Path-loss-for-isotrophic-source}}

This step starts by calculating which receiver points, $r$, are within
the specified transmission radius (see ``transmission radius'' in
\prettyref{fig:serial_version_flow_diagram}). For these points, the
LOS and NLOS conditions are calculated, with respect to the transmitter
(see ``Calculate LOS/NLOS'' in \prettyref{fig:serial_version_flow_diagram}).
The following step consists of calculating the path loss for an isotropic
source (or omni antenna). This calculation is performed by applying
the COST-231 path-loss model, which was previously introduced in Section
\ref{sub:COST-231-model}, to each of the points within the transmission
radius around the transmitter. Depending on whether the receiver point
$r$ is in LOS or NLOS, either Equation~(\ref{eq:cost231_LOS-1})
or Equation~(\ref{eq:cost231_NLOS-1}) is respectively applied (see
``Apply COST-231, LOS'' or ``Apply COST-231, NLOS'' in \prettyref{fig:serial_version_flow_diagram}).

\prettyref{fig:path_loss-example} shows a portion of a raster map
with an example result of the isotropic path-loss calculation. The
color scale is given in dB, indicating the signal loss from the isotropic
source, located in the center. Also, the hilly terrain is clearly
distinguished due to LOS and NLOS conditions from the signal source.

\begin{figure*}[tbh]
\begin{minipage}[t]{0.49\textwidth}%
\centering

\includegraphics[width=0.8\columnwidth]{img/isotrophic_calculation}

\caption{\textit{\emph{Example of raster map, showing the result of a path-loss
calculation from an isotropic source.\label{fig:path_loss-example}}}}
%
\end{minipage}\hfill{}%
\begin{minipage}[t]{0.49\textwidth}%
\centering

\includegraphics[width=0.8\columnwidth]{img/antenna_calculation}

\caption{\textit{\emph{Example of raster map, showing the antenna influence
over the isotropic path-loss result.\label{fig:antenna-example}}}}
%
\end{minipage}
\end{figure*}



\subsubsection{Antenna diagram influence\label{sub:Antenna-diagram-influence}}

This step considers the antenna radiation diagram of the current transmitter
and its influence over the isotropic path-loss calculation (see ``Calculate
antenna influence'' in \prettyref{fig:serial_version_flow_diagram}).
Working on the in-memory results generated by the previous step, the
radiation diagram of the antenna is taken into account, including
beam direction, electrical and mechanical tilt. \prettyref{fig:antenna-example}
shows a portion of a raster map, where this calculation step has been
applied to the results from \prettyref{fig:path_loss-example}. Notice
the distortion of the signal propagation that the antenna has introduced.


\subsubsection{Transmitter path-loss prediction\label{sub:Transmitter-path-loss-prediction}}

In this step, the coverage prediction of the transmitter is saved
in its own database table (see ``Save transmitter path-loss to DB''
in \prettyref{fig:serial_version_flow_diagram}), thus considerably
enhancing the write performance during the result-dumping phase, which
involves saving the path-loss results. This is accomplished by connecting
the standard output of the developed module with the standard input
of a database client. Naturally, the generated plain text should be
understood by the database server itself.


\subsubsection{Coverage prediction\label{sub:Final-coverage-prediction}}

The final radio coverage prediction, containing an aggregation of
the partial path-loss results of the involved transmitters, is created
in this step (see ``Create final coverage prediction'' in \prettyref{fig:serial_version_flow_diagram}).
The received signal strength from each of the transmitters is calculated
as the difference between its transmit power and path loss for the
receiver's corresponding position. This is done for each point in
the target area by executing an SQL query over the tables containing
the path-loss predictions of each of the processed transmitters.

Finally, the output raster is generated, using the GRASS built-in
modules $v.in.ascii$ and $v.to.rast$, which create a raster map
using the results of the above-mentioned query as input. The raster
map contains the maximum received signal strength for each individual
point, as shown in \prettyref{fig:output_raster_example}. In this
case, the color scale is given in dBm, indicating the received signal
strength from the transmitters.

\begin{figure}[tbh]
\centering

\includegraphics[width=0.7\columnwidth]{img/final_coverage}

\caption{\textit{\emph{Example of a raster map, displaying the final coverage
prediction of several transmitters over a geographical area. The color
scale is given in dBm, indicating the received signal strength. Darker
colors denote areas with reduced signal due to the shadowing effect
of the hilly terrain. \label{fig:output_raster_example}}}}
\end{figure}



\subsection{Multi-paradigm parallel programming}

The implementation methodology adopted for PRATO follows a multi-paradigm
parallel programming approach in order to fully exploit the resources
of each of the nodes in a computing cluster. To effectively use a
shared memory multi-processor, PRATO uses POSIX threads to implement
parallelism \citep{Butenhof_Programming.with.POSIX.threads:1997}.
By using POSIX threads, multiple threads can exist within the same
process while sharing its resources. For instance, an application
using POSIX threads can execute multiple threads in parallel by using
the cores of a multi-core processor, or use the system resources more
effectively, thus avoiding process execution-halt due to I/O latency
by using one thread for computing, while a second thread waits for
an I/O operation to complete. 

To use the computing resources of a distributed memory system, such
as a cluster of processors, PRATO uses the Message Passing Interface
(MPI) \citep{Gropp_Using_MPI:1999}. MPI is a message-passing standard,
which defines syntax and semantics designed to function on a wide
variety of parallel computers. MPI enables multiple processes running
on different processors of a computer cluster to communicate with
each other. It was designed for high performance on both massively
parallel machines and on workstation clusters. Its development is
supported by a broadly-based committee of vendors, developers, and
users.

In order to make the text more clear and to differentiate between
the programming paradigms used from here on, we will refer to a POSIX
thread simply as a `thread' and a MPI process as a `process'.


\subsection{Design of the parallel version\label{sub:Design-parallel}}

Keeping our focus on the usability and performance of PRATO, we are
introducing a new distributed implementation to overcome computational-time
constraints that prevented the reference implementation from tackling
big problem instances \citep{Ozimek_Open.source.radio.coverage.prediction:2010}.

Some authors have already published their work on implementing parallel
versions of GRASS modules for solving different time-consuming tasks
\citep{Akhter_Porting_GRASS_raster_module_to_distributed_computing:2007,Campos_Parallel_modelling_in_GIS:2012,Sorokine_Parallel_visualization_in_GRASS:2007}.
However, one major drawback of GRASS as a parallelization environment
is that it is not thread-safe, meaning that concurrent changes to
the same data set have undefined behavior. To overcome this problem,
we present a technique that saves the simulation results asynchronously
and independently from the GRASS environment, e.g. into an external
database system. This database system works also as an input source,
serving data to GRASS, whether it is used to aggregate the partial
results of the path-loss prediction or to visualize them. It is worth
pointing out that any database system may be used. By this we mean
relational, distributed \citep{Ozsu_Principles_of_distributed_database_systems:2011}
or even those of the NoSQL type \citep{Stonebraker_SQL_databases_vs_NoSQL_databases:2010}.
Nevertheless, in this study we use a central relational database system,
since they are the most popular and widely available ones.

We also introduce a methodology that allows the parallel implementation
to be almost completely GRASS independent. This means that a GRASS
installation is needed on only one of the nodes, i.e. the master node
of the target computer cluster, thus improving the usability of the
introduced methods. Also, a message-passing technique is proposed
to distribute the work-load among nodes hosting the worker processes.
Using this technique, computing nodes featuring more capable hardware
receive more work than those with weaker configurations, thus ensuring
a better utilization of the available computing resources despite
hardware diversity.


\subsubsection{Master process\label{sub:Master-process}}

\begin{figure}[b]
\begin{minipage}[t]{0.49\textwidth}%
\centering

\includegraphics[width=0.48\columnwidth]{img/master_process_flow_diagram}

\caption{\textit{\emph{Flow diagram of the master process.\label{fig:master_process}}}}
%
\end{minipage}\hfill{}%
\begin{minipage}[t]{0.49\textwidth}%
\centering

\includegraphics[width=1\columnwidth]{img/master_processing_loop_flow_diagram}

\caption{\textit{\emph{Flow diagram of the ``Processing loop'' step of the
master process.\label{fig:processing_loop_in_master_process}}}}
%
\end{minipage}
\end{figure}


As it has been suggested before, the parallel version of PRATO follows
a master-worker model. The master process, for which the flow diagram
is given in \prettyref{fig:master_process}, is the only component
that should be run from within the GRASS environment. As soon as the
master process starts, the input parameters are read. This step corresponds
to ``Read input data'' in \prettyref{fig:master_process}, and it
is done in a similar way as in the serial version. In the next step,
the master process dynamically initiates the worker processes using
the available computing nodes (see ``Dynamic worker-process spawning''
in \prettyref{fig:master_process}), based on the amount of transmitters
(or points of interest) for which the coverage prediction should be
calculated. This means that master process never starts more worker
processes than there are transmitters to be processed. However, if
the number of transmitters is larger than the amount of available
computing nodes, the master process can assign several transmitters
to each of the worker processes. For distributing the work among the
worker processes, the master process proceeds to decompose the loaded
raster data into arrays of basic-data-type elements, e.g. floats or
doubles, before dispatching them to the multiple worker processes
(see ``Input data broadcasting'' in \prettyref{fig:master_process}).
In this case, the decomposition of the data applies to the digital-elevation
and the clutter data only, but it could be applied to any point-based
data set, vector or raster. In the next step, the master process starts
a message-driven processing loop (see ``Processing loop'' in \prettyref{fig:master_process}),
which main task is to assign and distribute the configuration data
of different transmitters among idle worker processes. If using points
of interest instead of transmitters, this configuration data translates
into local properties that differ among points of interest.

The flow diagram shown in \prettyref{fig:processing_loop_in_master_process}
depicts in more detail the steps inside the ``Processing loop''
step of the master process. In the processing loop, the master process
starts by checking the available worker processes, which will calculate
the radio coverage prediction for the next transmitter. It is worth
pointing out that this step also serves as a stopping condition for
the processing loop itself (see ``Any worker still on?'' in \prettyref{fig:processing_loop_in_master_process}).
The active worker processes inform the master process they are ready
to compute by sending an idle message (see ``Wait for idle worker''
in \prettyref{fig:processing_loop_in_master_process}). The master
process then announces the idle worker process it is about to receive
new data for the next calculation, and it dispatches the complete
configuration of the transmitter to be processed (see ``Send keep-alive
message'' and ``Send transmitter data'' steps, respectively, in
\prettyref{fig:processing_loop_in_master_process}). This is only
done in case there are transmitters for which the coverage prediction
has yet to be calculated (see ``Any transmitters left?'' in \prettyref{fig:processing_loop_in_master_process}).
The processing loop of the master process continues to distribute
transmitter data among worker processes, which asynchronously become
idle as they finish the coverage-prediction calculations for the transmitters
they have been assigned by the master process. When there are no more
transmitters left, all the worker processes announcing they are idle
will receive a shutdown message from the master process, indicating
them to stop running (see ``Send stop message'' in \prettyref{fig:processing_loop_in_master_process}).
The master process will keep doing this until all worker processes
have finished (see ``Any worker still on?'' in \prettyref{fig:processing_loop_in_master_process}),
thus fulfilling the stopping condition of the processing loop.

Finally, the last step of the master process is devoted to creating
the final output of the calculation, e.g. a raster map (see ``Create
final coverage prediction'' in \prettyref{fig:master_process}).
The final coverage prediction of all transmitters is an aggregation
from the individual path-loss results created by each of the worker
processes during the ``Processing loop'' phase in \prettyref{fig:master_process},
which provides the source data for the final raster map. The aggregation
of the individual transmitter path-loss results is accomplished by
issuing an SQL query over the database tables containing the partial
results, in a similar way as in the serial version.


\subsubsection{Worker processes}

An essential characteristic of the worker processes is that they are
completely independent from GRASS, i.e. they do not have to run within
the GRASS environment nor use any of the GRASS libraries to work.
This aspect significantly simplifies the deployment phase to run PRATO
on a computer cluster, since no GRASS installation is needed on the
computing nodes hosting the worker processes.

The computations of the worker processes, for which the flow diagram
is given in \prettyref{fig:worker_process_flow_diagram}, are initialized
by data that are received from the master process at initialization
time (see ``Receive broadcasted data'' in \prettyref{fig:worker_process_flow_diagram}).
It is important to note that the received data contain the transmitter
and terrain-profile information which is common to all the coverage-prediction
calculations, therefore making each worker process capable of processing
any given transmitter.

The reason for the worker processes to be independent from GRASS arises
from the design of GRASS itself. Specifically, the existing GRASS
library, distributed with the GRASS GIS package, is not thread-safe,
because GRASS was designed as a system of small stand-alone modules
and not as a library for multi-threaded programs \citep{Blazek_GRASS_server:2004}.
Because of this limitation, it is not an option for a parallel implementation
to create separate threads for each worker process, since this would
mean worker processes should wait for each other to finish, before
accessing the target data. Consequently, the scalability of such implementation
would be very limited.

One possible solution to overcome this limitation would be to save
the transmitter path-loss prediction result through the master process,
thus avoiding concurrent access. However, sending intermediate results
back to the master process from the workers would represent a major
bottleneck for the scalability of the parallel version, since the
results generated by a parallel computation would have to be serially
processed by the master process alone. Instead, our approach allows
each of the worker processes to output its results into an external
database server, following an asynchronous and decoupled design. Each
of the transmitter path-loss prediction results are saved in separate
tables, i.e. one for each transmitter or point of interest. Moreover,
worker processes do this from an independent thread, which runs concurrently
with the calculation of the next transmitter received from the master
process. The overlap between calculation and communication achieved
by the use of an auxiliary thread completely hides the latency created
by the result dumping task, and makes better use of the system resources.

After the broadcasted data are received by all the worker processes,
each worker process proceeds to inform the master process that it
is ready (i.e. in an idle state) to receive the transmitter-configuration
data that defines which transmitter path-loss prediction to perform
(see ``Send idle message'' in \prettyref{fig:worker_process_flow_diagram}).
If the master process does not instruct to stop processing (see ``Has
stop message arrived?'' in \prettyref{fig:worker_process_flow_diagram}),
the worker process collects the transmitter configuration sent (see
``Receive transmitter data'' in \prettyref{fig:worker_process_flow_diagram}).
However, in case a stop message is received, the worker process will
wait for result-dumping threads to finish (see ``Wait for result-dump
threads'' in \prettyref{fig:worker_process_flow_diagram}) before
shutting down. The coverage calculation itself follows a similar design
as the serial version (see ``Coverage calculation'' in \prettyref{fig:worker_process_flow_diagram})
and it is executed for the received transmitter or point of interest.

As it was mentioned before, the worker process launches an independent
thread to save the path-loss prediction of the target transmitter
to a database table (see ``Threaded save path-loss to DB'' in \prettyref{fig:worker_process_flow_diagram}).
It is important to note that there is no possibility of data inconsistency
due to the saving task being executed inside a thread, since path-loss
data from different workers belong to different transmitters and are,
at least at this point of the process, mutually exclusive.

\begin{figure*}[tbh]
\begin{minipage}[t]{0.49\textwidth}%
\centering

\includegraphics[width=0.85\columnwidth]{img/worker_process_flow_diagram}

\caption{\textit{\emph{Flow diagram of a worker process.\label{fig:worker_process_flow_diagram}}}}
%
\end{minipage}\hfill{}%
\begin{minipage}[t]{0.49\textwidth}%
\centering

\includegraphics[width=1\columnwidth]{img/master_worker_communication_diagram}

\caption{\textit{\emph{Communication diagram, showing message passing between
master and one worker process.\label{fig:master_worker_communication}}}}
%
\end{minipage}
\end{figure*}



\subsubsection{Master-worker communication\label{sub:Master-worker-communication}}

The selected message-passing technique introduced in this work enables
a better use of the available computing resources, both in terms of
scalability and load balancing. This argument is supported by the
experimental results, introduced in Section~\ref{sec:Simulations}.

The first reason to implement the message-passing technique is to
support heterogeneous computing environments. In particular, our approach
focuses on taking full advantage of the hardware of each computing
node, thus explicitly avoiding the possible bottlenecks introduced
by the slowest computing node in the cluster. In other words, computing
nodes that deliver better performance get more calculations assigned
to the worker processes they host. The main advantages of this technique
are simplicity and negligible overhead, which contrast with more elaborated
approaches for parallel-task allocation in heterogeneous clusters
\citep{Bosque_A_parallel_computational_model_for_heterogenous_clusters:2006}.

A second reason for selecting a message-passing technique is related
to the flexibility it provides for load balancing, which is of great
importance on heterogeneous clusters. This can be seen in \prettyref{fig:worker_process_flow_diagram}
where the master process, before delivering the transmitter-configuration
data, sends a message to the worker process indicating that it is
about to receive more work. This a priori meaningless message has
a key role in correctly supporting computer clusters. In general,
there are many different ways a parallel program can be executed,
because the steps from the different processes can be interleaved
in various ways and a process can make non-deterministic choices \citep{Siegel_Verification_of_halting_properties_for_MPI_programs:2007},
which may lead to situations such as race conditions \citep{Clemencon_MPI_Race_detection:1995}
and deadlocks. A deadlock occurs whenever two or more running processes
are waiting for each other to finish, and thus neither ever does.
To prevent PRATO from deadlocking, message sending and receiving should
be paired, being equal number of send and receive messages on the
master and worker sides \citep{Siegel_Verification_of_halting_properties_for_MPI_programs:2007}.

\prettyref{fig:master_worker_communication} depicts a diagram of
the master-worker message passing, from which the transmitter-data
transmission has been excluded for clarity. Note how each idle message
sent from the worker process is paired with an answer from the master
process, whether it is a keep-alive or a stop message.


\section{Simulations \label{sec:Simulations}}

This section presents the simulations and analysis of the parallel
version of PRATO. Our aim is to provide an exhaustive analysis of
the performance and scalability of the parallel implementation in
order to determine if the objectives of this work are fulfilled. The
most common usage case for PRATO is to perform a radio-coverage prediction
for multiple transmitters, therefore, a straight forward parallel
decomposition is to divide a given problem instance by transmitter,
for which each coverage prediction is calculated by a separate worker
process.

The following simulations were carried out on 34 computing nodes of
the DEGIMA cluster. The computing nodes are connected by a LAN, over
a Gigabit Ethernet interconnect. As it has been mentioned before,
the reason for using a high-end computer cluster as DEGIMA is to explore
by experimentation the advantages and drawbacks of the introduced
methods. However, this does not imply any loss of generality when
applying these principles over a different group of networked computer,
acting as a computer cluster.

Each computing node of DEGIMA features one of two possible configurations,
namely:
\begin{itemize}
\item Intel Core i5-2500T quad-core processor CPU, clocked at 2.30 GHz,
with 16 GB of RAM; and
\item Intel Core i7-2600K quad-core processor CPU, clocked at 3.40 GHz,
also with 16 GB of RAM.
\end{itemize}
During the simulation runs, the nodes equipped with the Intel i5 CPU
host the worker processes, whereas the master process and the PostgreSQL
database server (version 9.1.4) run each on a different computing
node, featuring an Intel i7 CPU. The database server performs all
its I/O operations on the local file system, which is mounted on a
8~GB RAM disk.

All nodes are equipped with a Linux 64-bit operating system (Fedora
distribution). As the message passing implementation we use OpenMPI,
version 1.6.1, which has been manually compiled with the distribution-supplied
gcc compiler, version 4.4.4.


\subsection{Test networks}

To test the parallel performance of PRATO, we have prepared different
problem instances that emulate real radio networks of different sizes.
In order to create synthetic test data-sets with an arbitrary number
of transmitters, we use the data of a group of 10 transmitters, which
we randomly replicate and distribute over the whole target area. The
configuration parameters of these 10 transmitters were taken from
the LTE network deployed in Slovenia by Telekom Slovenije, d.d. The
path-loss predictions are calculated using the COST-231. The digital
elevation model has an area of 20,270~km$^{2}$, with a resolution
of 25~m$^{2}$. The clutter data extends over the same area with
the same resolution, and it contains different levels of signal loss
based on the land usage. For all the points within a radius of 20~km
around each transmitter, we assume that the receiver is positioned
1.5~m above the ground, and the frequency is set to 1,843~MHz.


\subsection{Weak scalability}

This set of simulations is meant to analyze the scalability of the
parallel implementation in cases where the workload assigned to each
process (one MPI process per processor core) remains constant as we
increase the number of processor cores and the total size of the problem,
i.e. the number of transmitters deployed over the target area is directly
proportional to the number of processor cores and worker processes.
We do this by assigning a constant number of transmitters per core
while increasing the number of cores hosting the worker processes.
Consequently, we tackle larger radio-network instances as we increase
the number of cores. Here we test for the following numbers of transmitters
per worker/core: $\{5,10,20,40,80\}$, by progressively doubling the
number of workers per core from 1 to 128.

Problems particularly well-suited for parallel computing exhibit computational
costs that are linearly dependent on the problem size. This property,
also referred to as algorithmic scalability, means that proportionally
increasing both the problem size and the number of cores results in
a roughly constant time to solution. Therefore, with this set of experiments,
we would like to investigate how well-suited the coverage-prediction
problem is for parallel computing environments.


\subsubsection{Results and discussion}

The results collected after the simulations for the weak-scalability
experiments are shown in Table \ref{tab:results_weak_scaling}. All
measurements express wall-clock times in seconds for each problem
instance, defined as number of transmitters per core (TX/core). Wall-clock
time represents real time that elapses from the start of the master
process to its end, including time that passes waiting for resources
to become available. They are plotted in \prettyref{fig:weak_scalability_time}.

\begin{table}[tbh]
\centering

\caption{\textit{\emph{Execution wall-clock times (in seconds) of the simulations
for the weak-scalability experiments.\label{tab:results_weak_scaling}}}}


{\footnotesize }%
\begin{tabular}{ccccccccc}
\cmidrule{2-9} 
 & \multicolumn{8}{c}{{\footnotesize Number of cores}}\tabularnewline\addlinespace
\midrule 
{\footnotesize TX/core} & {\footnotesize 1} & {\footnotesize 2} & {\footnotesize 4} & {\footnotesize 8} & {\footnotesize 16} & {\footnotesize 32} & {\footnotesize 64} & {\footnotesize 128}\tabularnewline
\midrule
{\footnotesize 5} & {\footnotesize 92} & {\footnotesize 99} & {\footnotesize 118} & {\footnotesize 122} & {\footnotesize 123} & {\footnotesize 124} & {\footnotesize 125} & {\footnotesize 126}\tabularnewline
{\footnotesize 10} & {\footnotesize 140} & {\footnotesize 152} & {\footnotesize 171} & {\footnotesize 175} & {\footnotesize 177} & {\footnotesize 179} & {\footnotesize 180} & {\footnotesize 182}\tabularnewline
{\footnotesize 20} & {\footnotesize 244} & {\footnotesize 260} & {\footnotesize 278} & {\footnotesize 282} & {\footnotesize 284} & {\footnotesize 285} & {\footnotesize 287} & {\footnotesize 290}\tabularnewline
{\footnotesize 40} & {\footnotesize 451} & {\footnotesize 470} & {\footnotesize 491} & {\footnotesize 497} & {\footnotesize 500} & {\footnotesize 502} & {\footnotesize 504} & {\footnotesize 509}\tabularnewline
{\footnotesize 80} & {\footnotesize 865} & {\footnotesize 892} & {\footnotesize 920} & {\footnotesize 925} & {\footnotesize 928} & {\footnotesize 931} & {\footnotesize 937} & {\footnotesize 948}\tabularnewline
\bottomrule
\end{tabular}
\end{table}


\begin{figure}[tbh]
\centering

\includegraphics[width=0.7\columnwidth]{img/weak_scaling-time_plot}

\caption{\textit{\emph{Measured wall-clock time for weak-scalability experiments
as shown in Table \ref{tab:results_weak_scaling}.}}\textit{ }\textit{\emph{Experiments
performed assigned one MPI worker process per available core. The
wall-clock time axis is expressed in base-10 logarithmic scale, whereas
the axis representing the number of cores is expressed in base-2 logarithmic
scale.\label{fig:weak_scalability_time}}}}
\end{figure}


The time measurements observed from the weak-scalability results show
that the wall-clock times do not grow rapidly, especially when the
number of cores is more than 8. Moreover, these times are almost constant
for bigger problem instances, revealing that the achieved level of
scalability gets close-to-linear as the amount of transmitters-per-core
increases. Certainly, the parallel version of PRATO scales especially
well when challenged with a big number of transmitters (10,240 for
the biggest instance) over 128 cores. This fact shows PRATO would
be able to calculate the radio coverage prediction for real networks
in a feasible amount of time, since many operational radio networks
have already deployed a comparable number of transmitters, e.g. the
3G network within the Greater London Authority area, in the UK \citep{Number_of_base_stations_in_England}.

\begin{figure}[b]
\begin{minipage}[t]{0.48\textwidth}%
\centering

\includegraphics[width=1\columnwidth]{img/weak_scaling_relative_time_plot_10}%
\end{minipage}\hfill{}%
\begin{minipage}[t]{0.48\textwidth}%
\centering

\includegraphics[width=1\columnwidth]{img/weak_scaling_relative_time_plot_20}%
\end{minipage}\\
\begin{minipage}[t]{0.48\textwidth}%
\centering

\includegraphics[width=1\columnwidth]{img/weak_scaling_relative_time_plot_40}%
\end{minipage}\hfill{}%
\begin{minipage}[t]{0.48\textwidth}%
\centering

\includegraphics[width=1\columnwidth]{img/weak_scaling_relative_time_plot_80}%
\end{minipage}

\caption{Relative times for the weak-scalability experiments.\emph{ }\textit{\emph{The
relative-processing time axes are expressed in linear scale, whereas
the axes representing the number of cores are expressed in base-2
logarithmic scale.\label{fig:weak_scaling-relative_times}}}}
\end{figure}


Not being able to achieve perfect weak scalability is due to a number
of factors. Specifically, the overhead time of the serial sections
of the master process grow proportionally with the number of cores,
although the total contribution of this overhead remains low for large
problem sizes. Moreover, the communication overhead grows linearly
with the number of cores used.

To confirm these arguments, we analyze the times of each of the steps
taken by the master process relative to the total processing time.
To this end, we have created plots for four problem instances 10,
20, 40 and 80 transmitters per core, which are shown in \prettyref{fig:weak_scaling-relative_times}.
The relative-processing time plots follow the formula

\begin{equation}
RT=\frac{t_{\textrm{rd}}+t_{\textrm{ps}}+t_{\textrm{db}}+t_{\textrm{pl}}+t_{\textrm{cp}}}{t_{\textrm{total}}},\label{eq:relative_processing_time}
\end{equation}


\noindent where $t_{\textrm{rd}}$ is the ``Read input data'' wall-clock
time, $t_{\textrm{ps}}$ is the wall-clock time of the ``Dynamic
worker-process spawning'' step, $t_{\textrm{db}}$ is the wall-clock
time of the ``Input data broadcasting'' step, $t_{\textrm{pl}}$
is the wall-clock time of the ``Processing loop'' step, $t_{\textrm{cp}}$
is the wall-clock time of the ``Create final coverage prediction''
step, and $t_{\textrm{total}}$ is the total wall-clock processing
time. For a reference of the different steps taking part of the master
process, see \prettyref{fig:master_process}.

From the relative-times plots, we see that, as we increase the number
of nodes, the largest fraction of the run-time is spent on the parallel
processing of transmitters, which scales notably well for larger problem
instances. The plotted relative times show that there is no dependency
between the relative processing times and the number of cores used,
confirming the good weak-scalability properties noted before. Additionally,
in all three plots we may observe a ``jump'' in the relative time
for the ``Input data broadcasting'' step that takes place when comparing
the result from 4 to 8 cores, i.e. from one to two computing nodes,
as each node hosts ``1 worker per core'' or a total of ``4 workers
per node''. This ``jump'' is due to the use of network communication
when more than one computing node participates in the parallel processing.
In addition, we may also conclude that the network infrastructure
has not been saturated with the data-passing load, since the relative
times for input-data broadcasting do not grow exponentially from 8
cores onward. Regarding the ``Create final coverage prediction''
step, we may see that as we increase the number of cores the relative
times grow proportionally for all three problem sizes.


\subsection{Strong scalability\label{sub:Strong-scalability}}

This set of simulations is meant to analyze the impact of increasing
the number of computing cores for a given problem size, i.e. the number
of transmitters deployed over the target area does not change, while
only the number of cores used is increased. Here we test for the following
number of transmitters: $\{64,128,256,512,1024,2048,4096\}$, by gradually
doubling the number of workers per core from 1 to 128 for each problem
size.


\subsubsection{Results and discussion}

The results of the time measurements collected after the simulations
for the strong-scalability experiments are shown in Table \ref{tab:results_strong_scaling}.
All times are expressed in seconds. These wall-clock time measurements
are plotted in \prettyref{fig:strong_scalability_time}\textit{\emph{.}}

\begin{table}[tbh]
\caption{\textit{\emph{Execution wall-clock times (in seconds) of the simulations
for the strong-scalability experiments.\label{tab:results_strong_scaling}}}}


\centering

{\footnotesize }%
\begin{tabular}{cccccccc}
\cmidrule{2-8} 
 & \multicolumn{7}{c}{{\footnotesize Number of transmitters}}\tabularnewline\addlinespace
\midrule 
{\footnotesize No. cores} & {\footnotesize 64} & {\footnotesize 128} & {\footnotesize 256} & {\footnotesize 512} & {\footnotesize 1024} & {\footnotesize 2048} & {\footnotesize 4096}\tabularnewline
\midrule
{\footnotesize 1} & {\footnotesize 714} & {\footnotesize 1392} & {\footnotesize 2740} & {\footnotesize 5437} & {\footnotesize 10830} & {\footnotesize 21562} & {\footnotesize 43217}\tabularnewline
{\footnotesize 2} & {\footnotesize 386} & {\footnotesize 734} & {\footnotesize 1419} & {\footnotesize 2791} & {\footnotesize 5535} & {\footnotesize 10996} & {\footnotesize 21987}\tabularnewline
{\footnotesize 4} & {\footnotesize 232} & {\footnotesize 408} & {\footnotesize 751} & {\footnotesize 1432} & {\footnotesize 2811} & {\footnotesize 5549} & {\footnotesize 11042}\tabularnewline
{\footnotesize 8} & {\footnotesize 155} & {\footnotesize 242} & {\footnotesize 409} & {\footnotesize 754} & {\footnotesize 1441} & {\footnotesize 2817} & {\footnotesize 5549}\tabularnewline
{\footnotesize 16} & {\footnotesize 113} & {\footnotesize 156} & {\footnotesize 244} & {\footnotesize 414} & {\footnotesize 759} & {\footnotesize 1447} & {\footnotesize 2821}\tabularnewline
{\footnotesize 32} & {\footnotesize 92} & {\footnotesize 114} & {\footnotesize 159} & {\footnotesize 245} & {\footnotesize 414} & {\footnotesize 760} & {\footnotesize 1449}\tabularnewline
{\footnotesize 64} & {\footnotesize 82} & {\footnotesize 94} & {\footnotesize 115} & {\footnotesize 159} & {\footnotesize 245} & {\footnotesize 420} & {\footnotesize 764}\tabularnewline
{\footnotesize 128} & {\footnotesize -} & {\footnotesize 83} & {\footnotesize 94} & {\footnotesize 116} & {\footnotesize 159} & {\footnotesize 248} & {\footnotesize 423}\tabularnewline
\bottomrule
\end{tabular}
\end{table}


\begin{figure}[tbh]
\centering

\includegraphics[width=0.7\columnwidth]{img/strong_scaling-time_plot}

\caption{\textit{\emph{Measured wall-clock time for strong-scalability experiments
as shown in Table \ref{tab:results_strong_scaling}. Experiments performed
assigned one MPI worker process per available core. The wall-clock
time axis is expressed in base-10 logarithmic scale, whereas the axis
representing the number of cores is expressed in base-2 logarithmic
scale.\label{fig:strong_scalability_time}}}}
\end{figure}


The time measurements show that small problem sizes per core have
a relatively large proportion of serial work and communication overhead.
Therefore, the performance deteriorates as the number of transmitters
per core approaches one. It can be observed in \prettyref{fig:strong_scalability_time}
that as we increase the number of transmitters used to solve a given
problem size, the slope of the curve generated by the progression
of wall-clock times tends to a flat line, i.e. as we increase the
number of transmitters there is no reduction in computational time.
This idea is more clearly noted in the test with smaller problem instances,
e.g. 64, 128 and 256 transmitters. In contrast, for the problems with
a number of transmitters larger than 512, the relative contribution
of the non-parallel steps to the wall-clock time is smaller, and a
larger portion of the time is spent on computing the transmitters
coverage in parallel (see Section \ref{sub:Design-parallel} for details
on the steps of PRATO algorithm). A more detailed discussion of the
reasons for the loss of parallel efficiency will be presented towards
the end of this section.

In order to observe how well the application scales when compared
against a base case, we have also measured the performance of the
parallel implementation in terms of the speedup, which is defined
as

\begin{equation}
S(NP)=\frac{execution\, time\, for\, base\, case}{execution\, time\, for\, NP\, cores},\label{eq:speedup}
\end{equation}


\noindent where $NP$ is the number of cores executing the worker
processes. As the base case for comparisons we have chosen the parallel
implementation running on only one core and decided against using
the serial implementation. We consider that the serial implementation
is not a good base comparison for the parallel results as it does
not reuse resources between each transmitter coverage calculation
and it does not overlap I/O operations with transmitter computations.
In practice, this means that several concatenated runs of the serial
version would be considerably slower than the parallel but single
worker implementation, because the serial implementation is not able
to use all of the memory bandwidth and computing resources simultaneously.
Therefore such comparison would be entirely biased towards the parallel
implementation, showing super-linear scaling and speedups which would
not be real, as the parallel version is better equipped to make use
of the system resources by means of multiple threads.

\begin{figure}[tbh]
\begin{minipage}[t]{0.48\textwidth}%
\centering

\includegraphics[width=1\columnwidth]{img/strong_scaling-speedup_plot}

\caption{\textit{\emph{Measured speedup for strong-scalability experiments.}}\textit{
}\textit{\emph{The speedup axis is expressed in base-2 logarithmic
scale, whereas the axis representing the number of cores is expressed
in base-2 logarithmic scale.\label{fig:strong_scalability_speedup}}}}
%
\end{minipage}\hfill{}%
\begin{minipage}[t]{0.48\textwidth}%
\centering

\includegraphics[width=1\columnwidth]{img/strong_scaling-efficiency_plot}

\caption{\textit{\emph{Measured parallel efficiency for strong-scalability
experiments.}}\textit{ }\textit{\emph{The parallel-efficiency axis
is expressed in linear scale, whereas the axis representing the number
of cores is expressed in base-2 logarithmic scale.\label{fig:strong_scalability_efficiency}}}}
%
\end{minipage}
\end{figure}


Using the speedup metric, linear scaling is achieved when the obtained
speedup is equal to the total number of processors used. However,
it should be noted that perfect speedup is almost never achieved,
due to the existence of serial stages within an algorithm and communication
overheads of the parallel implementation \citep{Cruz_Particle.Flow.Simulation:2010}. 

\prettyref{fig:strong_scalability_speedup} shows the speedup of the
parallel implementation for up to 128 cores (running one worker process
per node), and compares seven different problem sizes with 64, 128,
256, 512, 1024, 2048 and 4096 transmitters deployed over the target
area. The number of transmitters used in these problem sizes are comparable
to several operational radio networks that have already been deployed
in England, e.g. Bedfordshire County with 69 base stations, Cheshire
County with 132 base stations, Hampshire County with 227 base stations,
West Midlands with 414 base stations, and Greater London Authority
with 1086 base stations \citep{Number_of_base_stations_in_England}.
Moreover, consider that it is common for a single base station to
host multiple transmitters. 

We can see that the significant reductions in wall-clock time for
large problem sizes shown in \prettyref{fig:strong_scalability_time}
are directly correlated with the speedup factors shown in \prettyref{fig:strong_scalability_speedup}.

\begin{figure*}[tbh]
\begin{minipage}[t]{0.48\textwidth}%
\centering

\includegraphics[width=1\columnwidth]{img/strong_scaling-relative_time_plot_64}%
\end{minipage}\hfill{}%
\begin{minipage}[t]{0.48\textwidth}%
\centering

\includegraphics[width=1\columnwidth]{img/strong_scaling-relative_time_plot_256}%
\end{minipage}\\
\begin{minipage}[t]{0.48\textwidth}%
\centering

\includegraphics[width=1\columnwidth]{img/strong_scaling-relative_time_plot_1024}%
\end{minipage}\hfill{}%
\begin{minipage}[t]{0.48\textwidth}%
\centering

\includegraphics[width=1\columnwidth]{img/strong_scaling-relative_time_plot_4096}%
\end{minipage}

\caption{Relative times for the strong-scalability experiments.\emph{ }\textit{\emph{The
relative-processing time axes are expressed in linear scale, whereas
the axes representing the number of cores are expressed in base-2
logarithmic scale.\label{fig:strong_scaling-relative_times}}}}
\end{figure*}


To study how well PRATO utilizes the available computing resources
we consider the parallel efficiency of the implementation, i.e. how
well the parallel implementation makes use of the available processor
cores. The definition of parallel efficiency is as follows:

\begin{equation}
E(NP)=\frac{S(NP)}{NP},
\end{equation}


\noindent where $S(NP)$ is the speedup as defined in Equation (\ref{eq:speedup}),
and $NP$ is the number of cores executing worker processes. \prettyref{fig:strong_scalability_efficiency}
shows the parallel efficiency of the parallel implementation for different
problem sizes as we increase the number of processing cores. 

The ideal case for a parallel application would be to utilize all
available resources, in which case the parallel efficiency would be
constantly equal to one as we increase the core count. From the plot
in \prettyref{fig:strong_scalability_efficiency}, we may observe
that the efficiency is less than one, hence the computational resources
are under utilized. In accordance to the previous analysis, the under
utilization of the computing resources is more significant for the
smaller problem sizes, where number of assigned transmitters per core
approaches one. This is due to the increased relative influence introduced
by serial and communication overheads, without which the parallel
implementation would not be feasible. On the other hand, the relative
time contribution of the serial and communication overheads is significantly
reduced as the work-load per core increases. Unsurprisingly, these
results confirm what it has previously been suggested during the weak-scaling
analysis, i.e. it is not worth parallelizing small problem instances
over a large number of nodes, since the time reduction due to computations
that make use of the extra parallel resources is surpassed by the
extra parallel initialization and communication overhead.

Similarly as in the weak-scaling test, we study the relative contribution
of each of the steps of the master process as we increase the number
of cores used for a fixed problem size. In this case, we have created
plots for four problem instances, namely 64, 256, 1024 and 4096 transmitters,
which are shown in \prettyref{fig:strong_scaling-relative_times}.
The relative times shown are calculated using the formula depicted
in Equation (\ref{eq:relative_processing_time}).

We may observe the non-parallel steps comprising ``Read input data'',
``Dynamic worker-process spawning'', ``Input data broadcasting''
and ``Final coverage prediction'' contribute with a larger portion
of time as we increase the number of cores, because the total wall-clock
processing time decreases. Additionally, the low parallel efficiency
for small problem sizes, particularly for 64 and 256 transmitters
(left-most plots in \prettyref{fig:strong_scaling-relative_times}),
is validated with the relative small proportion of the radio-coverage
calculation (``Processing loop'') compared to the serial steps of
the process.


\subsection{Load balancing}

In this section, we analyze the level of utilization of the computing
resources available at the computing nodes hosting the worker processes.
Computing-resource utilization is achieved by partitioning the computational
workload and data across all processors. Efficient workload distribution
strategies should be based on the processor speed, memory hierarchy
and communication network \citep{Clarke_Dynamic_load_balancing:2011}.

The parallel implementation of PRATO performs load-balancing using
point-to-point messages (see Section \ref{sub:Master-worker-communication})
between master and worker processes. When a worker process issues
an idle message (see ``Send idle message'' in \prettyref{fig:master_worker_communication}),
the worker process will not continue until the message arrives to
the master process. A similar situation occurs when the master process
signals a worker back, whether to indicate it to shutdown or to continue
working. Since the process-to-core mapping is one-to-one, blocking
messages typically waste processor cycles on a computing node \citep{Bhandarkar_Adaptive_load_balancing_for_MPI:2001}.
Specifically, we would like to verify the penalties that such synchronization
technique has on the scalability of the parallel implementation.

We evaluate the load empirically \citep{Watts_A_practical_approach_to_dynamic_load_balancing:1998}
by using the following metric as an indicator of the load balancing
among processes:

{\small 
\begin{equation}
LB(NP)=\frac{minimum\, execution\, time\, among\, NP\, cores}{processing\, loop\, time\, of\, master\, process},
\end{equation}
}{\small \par}

\noindent where $NP$ is the number of cores executing worker processes.
Taking the processing-loop time of the master process ensures we measure
the overhead of the message passing during the time the coverage prediction
is being executed by the workers. This means that the time measurement
is performed excluding the serial parts of the process, i.e. after
the common data have been broadcasted to all worker processes (``Input
data broadcasting'' in \prettyref{fig:master_process}), until the
beginning of the last step (``Create final coverage prediction''
in \prettyref{fig:master_process}).

High performance is achieved when all cores complete their work within
the same time, hence showing a load-balancing factor of one. On the
other hand, lower values indicate disparity between the run times
of the various worker processes sharing the parallel task, thus reflecting
load imbalance.


\subsubsection{Results and discussion}

For this set of experiments, we have chosen the same problem sizes
as for strong scalability in Section \ref{sub:Strong-scalability},
where the coverage predictions are calculated up-to 128 cores, running
on 32 computing nodes.

\begin{figure}[tbh]
\centering

\includegraphics[width=0.7\columnwidth]{img/strong_scaling-load_balancing_plot}

\caption{\textit{\emph{Load balancing among worker processes.\label{fig:load_balancing}}}}
\end{figure}


From the plot shown in \prettyref{fig:load_balancing}, it is clear
that the influence of the message-passing overhead over the processing
time is inversely proportional to the amount of work each worker process
receives. Additionally, for the biggest problem instances (1024, 2048
and 4096 transmitters), parallel-process execution times are within
95\% of a perfect load-balancing factor, and within 90\% for problem
sizes with 256 and 512 transmitters, showing a very good performance
of the dynamic task assignment, driven by our message-passing technique.
For problem instances of 64 and 128 transmitters, the parallel-process
times are within 80\% of the perfect load balancing, showing that,
as the number of transmitters per core approaches one, latencies introduced
by several hardware and OS-specific factors (e.g. TurboBoost, process
affinity, etc.) are influential over the total processing time. Particularly,
message-passing is not able to compensate these latencies as it is
executed only once per worker process.

It is worth pointing out that the very good load-balancing factors
shown here are not only merit of the message-passing technique. The
result dumping of partial path-loss predictions, performed by the
worker processes in a separate thread into an external database server,
prevents data synchronization from occurring at each iteration of
the parallel process, consequently improving the load-balancing factors
significantly.


\section{Related work \label{sec:Related-work}}

The reported results in \citep{Ozimek_Open.source.radio.coverage.prediction:2010}
show a comparable quality to those of a professional radio-planning
tool. Since the results of the conducted comparison tests showed identical
results between PRATO and this work, we may conclude that PRATO reaches
solutions of comparable quality to those of a professional tool. However,
a performance comparison with this work has not been carried out,
because it only deals with serial implementations. 



The task-parallelization problem within the GRASS environment has
been addressed by several authors in different works. For example,
in \citep{Campos_Parallel_modelling_in_GIS:2012}, the authors present
a collection of GRASS modules for watershed analysis. Their work concentrates
on different ways of slicing raster maps to take advantage of a potential
MPI implementation, but there are no guidelines for work replication.
Moreover, the hardware specification, on which the experiments have
been run, is missing, making it very difficult to build upon this
work.

On the field of high-performance computing, the authors of \citep{Akhter_Porting_GRASS_raster_module_to_distributed_computing:2007}
have presented implementation examples of a GRASS raster module, used
to process vegetation indexes for satellite images, for MPI and Ninf-G
environments. The main drawback with their methodology is the compulsory
use of GRASS libraries in all the computing nodes that take part in
the parallel calculation, making them more difficult to setup. Moreover,
the authors explicitly acknowledge a limitation in the performance
of their MPI implementation for big processing jobs. The restriction
appears due to the computing nodes being fixed to a specific spatial
range, since the input data are equally distributed among worker processes,
creating an obstacle for load balancing in heterogeneous environments.
It is worth pointing out that in the parallel implementation of PRATO
we specifically address this problem with our message-passing technique.

Similarly, in \citep{Huang_Cluster_based_parallel_GIS:2011}, the
parallel implementation of the inverse distance weighting interpolation
algorithm is presented. Although it is not explicitly noted, it can
be concluded that the computing nodes make use of the GRASS environment,
again making them more difficult to setup. Moreover, since the amount
of work is evenly distributed among all processes (including the master
one), their approach would also show decreased efficiency in heterogeneous
environments.

Some years ago, grid computing received a lot of attention as a way
of accessing the extra computational power needed for spatial analysis
of large data sets \citep{Armstrong_Using_a_computational_grid_for_geographic_information_analysis:2005,Vouk_Cloud_computing_issues_research_and_implementations:2008,Wang_A_cybergis_framework_for_the_synthesis_of_cyberinfrastructure_GIS_and_spatial_analysis:2010}.
However, several obstacles are still preventing this technology from
being widely used. Namely, its adoption requires not only hardware
and software compromises of the involved parts, but also a behavioral
change at the human level \citep{Armstrong_Using_a_computational_grid_for_geographic_information_analysis:2005}.

In \citep{Yin_A_framework_for_integrating_GIS_and_parallel_computing_for_spatial_control_problems_a_case_study_of_wildfire_dontrol:2012},
the authors present a parallel framework for GIS integration. Based
on the principle of spatial dependency, they lower the calculation
processing time by backing it with a knowledge database, delivering
the heavy calculation load to the parallel back-end if a specific
problem instance is not found in the database. There is an additional
effort to achieve the presented goals, since the implementation of
a fully functional GIS (or ``thick GIS'' as the authors call it)
is required on both the desktop client and in the parallel environment.
On the other hand, their implementation uses proprietary software,
somewhat reducing the possibilities of adaptation for different GIS.

In \citep{Gong_Parallel_agent_based_simulation_of_individual_level_spatial_interactions_within_a_multicore_computing_environment:2012},
the authors present an agent-based approach for simulating spatial
interactions. Their approach decomposes the entire landscape into
equally-sized regions, which are in turn processed by a different
core of a multi-core CPU. Despite this, the agent interaction at the
region border greatly affects the efficiency of the algorithm, showing
that an asynchronous approach, as the one used by PRATO, would greatly
improve the overall efficiency of the system. Although their work
is geared towards CPUs with a high number of cores instead of a computing
cluster, it would be interesting to apply a methodology as the one
presented in this study in order to push all agent communications
to shared memory, thus improving the efficiency at region borders.


\section{Conclusion}

We have presented PRATO, a functional parallel radio-coverage prediction
tool for radio networks. The tool, as well as the patterns for exploiting
the computing power of a group of networked computers, namely a computer
cluster, are intended to be used for spatial analysis and decision
support. The introduced techniques, combined with the use of a database
system, deliver a platform for parallel and asynchronous computation,
that is completely independent from the GIS used, in this case the
GRASS environment. Consequently, a GIS installation is needed on only
one of the nodes, thus simplifying the required system setup and greatly
enhancing the applicability of this methodology in different environments.

Also, the proposed message-passing technique fairly distributes the
work-load among nodes hosting the worker processes. Hence, computing
nodes featuring more capable hardware receive more work than those
with weaker configurations, thus maximizing utilization of the available
computing resources.

The extensive simulations results, performed on the DEGIMA cluster
of the Nagasaki Advanced Computing Center, have been analyzed to determine
the level of scalability of the implementation, as well as the impact
of the presented methods for parallel-algorithm design aimed at spatial-data
processing.

The conducted analysis shows that PRATO is able to calculate the radio-coverage
prediction of real-world mobile networks in a reduced amount of time
with a high scalability level. The promising results also show the
great potential of our approach to parallelize different time-consuming
tasks for GIS. Optimization problems, where thousands of simulations
take part of the evaluation step during an optimization process, are
also very good candidates for this approach. Still, further research
is needed to fully demonstrate this point.

Encouraged by the favorable results, further work will include abstracting
the introduced principles and methodology into a multi-purpose library
for GRASS GIS, which shall be published as free and open source software.
By implementing such tool for spatial problem solving and decision
support, we aim at completely validating the suitability and usefulness
of the presented methods.

Nevertheless, as PRATO is also a free and open-source software project%
\footnote{The source code is available for download from http://cs.ijs.si/benedicic/%
}, it can be readily modified and extended to support, for example,
other propagation models and post-processing algorithms. This characteristic
defines a clear advantage when compared to commercial and closed-source
tools.


\section*{}

\bibliographystyle{tGIS}
\bibliography{manuscript}

\end{document}
